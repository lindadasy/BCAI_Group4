{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "interaction_data = pd.read_csv('/mnt/c/Users/80753/Desktop/MNS_data_full(1)(1) (1).csv')\n",
    "article_data = pd.read_csv('/mnt/c/Users/80753/Desktop/articles_2020_flexible_difficulty.csv')\n",
    "\n",
    "\n",
    "interaction_data.fillna(interaction_data.median(), inplace=True)\n",
    "article_data.dropna(inplace=True)\n",
    "\n",
    "article_data.rename(columns={'id': 'article_id'}, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(interaction_data, article_data, on='article_id', how='inner')\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "merged_data['user_idx'] = user_encoder.fit_transform(merged_data['user_id'])\n",
    "\n",
    "item_encoder = LabelEncoder()\n",
    "merged_data['item_idx'] = item_encoder.fit_transform(merged_data['article_id'])\n",
    "\n",
    "num_users = merged_data['user_idx'].nunique()\n",
    "num_items = merged_data['item_idx'].nunique()\n",
    "\n",
    "numerical_features = ['reading_time', 'SleepHours', 'Tired', 'Excited', 'Motivated',\n",
    "                     'Depression', 'Anxiety', 'Extraversion', 'Agreeableness',\n",
    "                     'Conscientiousness', 'Neuroticism', 'OpennessToExperience',\n",
    "                     'HowOftenNews', 'TimeWeekNews','flexible_difficulty' ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merged_data[numerical_features] = scaler.fit_transform(merged_data[numerical_features])\n",
    "\n",
    "user_feature_cols = ['SleepHours', 'Tired', 'Excited', 'Motivated',\n",
    "                     'Depression', 'Anxiety', 'Extraversion', 'Agreeableness',\n",
    "                     'Conscientiousness', 'Neuroticism', 'OpennessToExperience',\n",
    "                     'HowOftenNews', 'TimeWeekNews']\n",
    "\n",
    "item_feature_cols = ['reading_time', 'flexible_difficulty']\n",
    "\n",
    "# target_col = 'likability'\n",
    "# merged_data['likability'] = merged_data['likability'].round().astype(int)\n",
    "\n",
    "# max_class = merged_data['likability'].max()\n",
    "# min_class = merged_data['likability'].min()\n",
    "# merged_data = merged_data[~merged_data['likability'].isin([min_class])]\n",
    "# merged_data['likability'] = merged_data['likability']*1/2\n",
    "# merged_data['likability_class'] = merged_data[target_col].round().astype(int) - merged_data[target_col].min()  # 假设评分从1开始\n",
    "\n",
    "# num_classes = merged_data['likability_class'].nunique()\n",
    "\n",
    "target_col = 'likability'\n",
    "merged_data['likability'] = merged_data['likability'].round().astype(int)\n",
    "\n",
    "merged_data['likability_class'] = (merged_data[target_col] > 2).astype(int)\n",
    "\n",
    "num_classes = merged_data['likability_class'].nunique()\n",
    "\n",
    "X = merged_data[user_feature_cols + item_feature_cols]\n",
    "y = merged_data['likability_class']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)  # 移动到设备\n",
    "\n",
    "\n",
    "#Train\n",
    "train_user_ids = torch.tensor(\n",
    "    merged_data.loc[X_train.index, 'user_idx'].values\n",
    ")\n",
    "train_item_ids = torch.tensor(\n",
    "    merged_data.loc[X_train.index, 'item_idx'].values\n",
    ")\n",
    "train_user_features = torch.tensor(\n",
    "    X_train[user_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "train_item_features = torch.tensor(\n",
    "    X_train[item_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "train_targets = torch.tensor(\n",
    "    y_train.values, dtype=torch.long\n",
    ")\n",
    "\n",
    "#VAL\n",
    "val_user_ids = torch.tensor(\n",
    "    merged_data.loc[X_val.index, 'user_idx'].values\n",
    ")\n",
    "val_item_ids = torch.tensor(\n",
    "    merged_data.loc[X_val.index, 'item_idx'].values\n",
    ")\n",
    "val_user_features = torch.tensor(\n",
    "    X_val[user_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "val_item_features = torch.tensor(\n",
    "    X_val[item_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "val_targets = torch.tensor(\n",
    "    y_val.values, dtype=torch.long\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    train_user_ids, train_item_ids,\n",
    "    train_user_features, train_item_features, train_targets\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    val_user_ids, val_item_ids,\n",
    "    val_user_features, val_item_features, val_targets\n",
    ")\n",
    "\n",
    "batch_size = 32  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "def add_noise(X, noise_level=0.5):\n",
    "    noise = np.random.normal(0, noise_level, X.shape)\n",
    "    X_noisy = X + noise\n",
    "    return X_noisy\n",
    "\n",
    "\n",
    "noise_ratio = 0.1  \n",
    "num_noisy_samples = int(len(X_train) * noise_ratio)\n",
    "\n",
    "np.random.seed(42)\n",
    "noisy_indices = np.random.choice(len(X_train), num_noisy_samples, replace=False)\n",
    "X_noisy = X_train.iloc[noisy_indices].copy()\n",
    "y_noisy = y_train.iloc[noisy_indices].copy()\n",
    "\n",
    "X_noisy[user_feature_cols + item_feature_cols] = add_noise(X_noisy[user_feature_cols + item_feature_cols].values)\n",
    "\n",
    "X_train_aug = pd.concat([X_train, X_noisy], axis=0)\n",
    "y_train_aug = pd.concat([y_train, y_noisy], axis=0)\n",
    "print(\"Original Data:\\n\", X_train)\n",
    "print(\"Noisy Data:\\n\", X_noisy)\n",
    "\n",
    "train_user_ids_aug = torch.tensor(\n",
    "    merged_data.loc[X_train_aug.index, 'user_idx'].values\n",
    ")\n",
    "train_item_ids_aug = torch.tensor(\n",
    "    merged_data.loc[X_train_aug.index, 'item_idx'].values\n",
    ")\n",
    "train_user_features_aug = torch.tensor(\n",
    "    X_train_aug[user_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "train_item_features_aug = torch.tensor(\n",
    "    X_train_aug[item_feature_cols].values, dtype=torch.float32\n",
    ")\n",
    "train_targets_aug = torch.tensor(\n",
    "    y_train_aug.values, dtype=torch.long\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_aug = TensorDataset(\n",
    "    train_user_ids_aug, train_item_ids_aug,\n",
    "    train_user_features_aug, train_item_features_aug, train_targets_aug\n",
    ")\n",
    "\n",
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Module\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items,\n",
    "                 user_feature_dim, item_feature_dim, num_classes, embed_dim=64, feature_dim=32):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        \n",
    "        self.user_feature_transform = nn.Linear(user_feature_dim, feature_dim)\n",
    "        self.item_feature_transform = nn.Linear(item_feature_dim, feature_dim)\n",
    "\n",
    "        \n",
    "        interaction_dim = feature_dim\n",
    "        input_dim = embed_dim * 2 + feature_dim * 2 + interaction_dim \n",
    "\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, user_id, item_id, user_features, item_features):\n",
    "        \n",
    "        user_embed = self.user_embedding(user_id)\n",
    "        item_embed = self.item_embedding(item_id)\n",
    "        \n",
    "        \n",
    "        user_feat_transformed = self.user_feature_transform(user_features)\n",
    "        item_feat_transformed = self.item_feature_transform(item_features)\n",
    "        \n",
    "        \n",
    "        interaction_features = user_feat_transformed * item_feat_transformed\n",
    "        \n",
    "        \n",
    "        x = torch.cat([\n",
    "            user_embed, item_embed,\n",
    "            user_feat_transformed, item_feat_transformed, interaction_features\n",
    "        ], dim=-1)\n",
    "        \n",
    "        \n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output  \n",
    "\n",
    "model = NCF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    user_feature_dim=len(user_feature_cols),\n",
    "    item_feature_dim=len(item_feature_cols),\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)  \n",
    "\n",
    "# early stop\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader_aug:  \n",
    "        user_id, item_id, user_feat, item_feat, target = batch\n",
    "        user_id, item_id = user_id.to(device), item_id.to(device)\n",
    "        user_feat, item_feat = user_feat.to(device), item_feat.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(user_id, item_id, user_feat, item_feat)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_aug)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    #VAL\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            user_id, item_id, user_feat, item_feat, target = batch\n",
    "            user_id, item_id = user_id.to(device), item_id.to(device)\n",
    "            user_feat, item_feat = user_feat.to(device), item_feat.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(user_id, item_id, user_feat, item_feat)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        user_id, item_id, user_feat, item_feat, target = batch\n",
    "        user_id, item_id = user_id.to(device), item_id.to(device)\n",
    "        user_feat, item_feat = user_feat.to(device), item_feat.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(user_id, item_id, user_feat, item_feat)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(target.cpu().numpy())\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[f'Predicted {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Actual {i}' for i in range(num_classes)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses,  label='Training loss')\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses,  label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies)+1), train_accuracies,  label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies)+1), val_accuracies,  label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full(model, user_ids, item_ids, user_features, item_features, targets, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(user_ids.to(device), item_ids.to(device), user_features.to(device), item_features.to(device))\n",
    "        loss = criterion(output, targets.to(device)).item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct = (predicted == targets.to(device)).sum().item()\n",
    "        accuracy = correct / targets.size(0)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "feature_names = ['user_id', 'item_id'] + user_feature_cols + item_feature_cols\n",
    "\n",
    "\n",
    "feature_importance = {feature: 0.0 for feature in feature_names}\n",
    "\n",
    "\n",
    "val_user_ids_all = val_user_ids\n",
    "val_item_ids_all = val_item_ids\n",
    "val_user_features_all = val_user_features\n",
    "val_item_features_all = val_item_features\n",
    "val_targets_all = val_targets\n",
    "\n",
    "\n",
    "val_user_ids_np = val_user_ids_all.cpu().numpy()\n",
    "val_item_ids_np = val_item_ids_all.cpu().numpy()\n",
    "val_user_features_np = val_user_features_all.cpu().numpy()\n",
    "val_item_features_np = val_item_features_all.cpu().numpy()\n",
    "val_targets_np = val_targets_all.cpu().numpy()\n",
    "\n",
    "# DataLoader\n",
    "def create_dataloader_np(user_ids, item_ids, user_features, item_features, targets, batch_size=32):\n",
    "    tensor_user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "    tensor_item_ids = torch.tensor(item_ids, dtype=torch.long)\n",
    "    tensor_user_features = torch.tensor(user_features, dtype=torch.float32)\n",
    "    tensor_item_features = torch.tensor(item_features, dtype=torch.float32)\n",
    "    tensor_targets = torch.tensor(targets, dtype=torch.long)\n",
    "    dataset = TensorDataset(tensor_user_ids, tensor_item_ids, tensor_user_features, tensor_item_features, tensor_targets)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "base_loss, base_acc = evaluate_full(model, val_user_ids_all, val_item_ids_all, val_user_features_all, val_item_features_all, val_targets_all, criterion, device)\n",
    "print(f'Baseline Validation Loss: {base_loss:.4f}, Baseline Validation Accuracy: {base_acc:.4f}')\n",
    "\n",
    "\n",
    "for feature in feature_names:\n",
    "    shuffled_user_ids = val_user_ids_np.copy()\n",
    "    shuffled_item_ids = val_item_ids_np.copy()\n",
    "    shuffled_user_features = val_user_features_np.copy()\n",
    "    shuffled_item_features = val_item_features_np.copy()\n",
    "    shuffled_targets = val_targets_np.copy()\n",
    "\n",
    "    if feature == 'user_id':\n",
    "       \n",
    "        shuffle_indices = np.random.permutation(len(shuffled_user_ids))\n",
    "        shuffled_user_ids = shuffled_user_ids[shuffle_indices]\n",
    "        shuffled_user_features = shuffled_user_features[shuffle_indices]\n",
    "    elif feature == 'item_id':\n",
    "        \n",
    "        shuffle_indices = np.random.permutation(len(shuffled_item_ids))\n",
    "        shuffled_item_ids = shuffled_item_ids[shuffle_indices]\n",
    "        shuffled_item_features = shuffled_item_features[shuffle_indices]\n",
    "    elif feature in user_feature_cols:\n",
    "        \n",
    "        shuffled_user_features[:, user_feature_cols.index(feature)] = np.random.permutation(shuffled_user_features[:, user_feature_cols.index(feature)])\n",
    "    elif feature in item_feature_cols:\n",
    "        \n",
    "        shuffled_item_features[:, item_feature_cols.index(feature)] = np.random.permutation(shuffled_item_features[:, item_feature_cols.index(feature)])\n",
    "    else:\n",
    "        print(f'Unknown feature: {feature}')\n",
    "        continue\n",
    "\n",
    "  \n",
    "    temp_loader = create_dataloader_np(shuffled_user_ids, shuffled_item_ids, shuffled_user_features, shuffled_item_features, shuffled_targets, batch_size=batch_size)\n",
    "\n",
    "    shuffled_loss, shuffled_acc = evaluate_full(model, torch.tensor(shuffled_user_ids, dtype=torch.long),\n",
    "                                               torch.tensor(shuffled_item_ids, dtype=torch.long),\n",
    "                                               torch.tensor(shuffled_user_features, dtype=torch.float32),\n",
    "                                               torch.tensor(shuffled_item_features, dtype=torch.float32),\n",
    "                                               torch.tensor(shuffled_targets, dtype=torch.long),\n",
    "                                               criterion, device)\n",
    "\n",
    "    # performance decrease\n",
    "    loss_increase = shuffled_loss - base_loss\n",
    "    acc_decrease = base_acc - shuffled_acc\n",
    "    feature_importance[feature] = acc_decrease  \n",
    "    print(f'Feature: {feature}, Accuracy Decrease: {acc_decrease:.4f}, Loss Increase: {loss_increase:.4f}')\n",
    "\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance (based on accuracy decrease):\")\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
